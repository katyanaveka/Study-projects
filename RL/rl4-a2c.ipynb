{"metadata":{"celltoolbar":"Отсутствует","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementing Advantage-Actor Critic (A2C) - 2 pts","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install comet_ml --quiet\n!pip install tensorboardX --quiet\n!pip install setuptools==65.5.0\n!pip install gym[atari,accept-rom-license]==0.21.0","metadata":{"execution":{"iopub.status.busy":"2023-04-16T10:51:18.122633Z","iopub.execute_input":"2023-04-16T10:51:18.123691Z","iopub.status.idle":"2023-04-16T10:52:31.368321Z","shell.execute_reply.started":"2023-04-16T10:51:18.123649Z","shell.execute_reply":"2023-04-16T10:52:31.366964Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, '/kaggle/input/utils')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-16T12:14:21.612599Z","iopub.execute_input":"2023-04-16T12:14:21.613700Z","iopub.status.idle":"2023-04-16T12:14:21.619738Z","shell.execute_reply.started":"2023-04-16T12:14:21.613641Z","shell.execute_reply":"2023-04-16T12:14:21.618543Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"from comet_ml import Experiment\nfrom comet_ml.integration.pytorch import log_model\nfrom tensorboardX import SummaryWriter\n\n\nexperiment = Experiment(\n  api_key = \"KA73UNUPM3eR56uID83Ii8HH4\",\n  project_name = \"actor_critic_gae\",\n  workspace=\"katyanaveka\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:24.533584Z","iopub.execute_input":"2023-04-16T15:29:24.534292Z","iopub.status.idle":"2023-04-16T15:29:29.073508Z","shell.execute_reply.started":"2023-04-16T15:29:24.534254Z","shell.execute_reply":"2023-04-16T15:29:29.072228Z"},"trusted":true},"execution_count":459,"outputs":[{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/katyanaveka/general/01e1c70662784aa4bd5c7216a4cfd156\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Episodes/episode_length [442]           : (137.5, 594.75)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Episodes/max_reward [442]               : (125.0, 825.0)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Episodes/min_reward [442]               : (5.0, 135.0)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Episodes/reward_mean_100 [442]          : (138.83460968379447, 182.5)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Episodes/total_reward [442]             : (85.625, 301.875)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     critic/advantage [28697]                : (1.104797498637171e-05, 13.328714594494608)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     critic/values_value predictions [28697] : (0.04772196337580681, 62.49842071533203)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     critic/values_value targets [28697]     : (0.04475554593276513, 59.39929709811715)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gradient_norm [28697]                   : (0.23242151737213135, 14.929691314697266)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses_critic loss [28697]              : (1.104797498637171e-05, 13.328714594494608)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses_entropy loss [28697]             : (-28.82382583618164, -0.0006983845960348845)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses_policy loss [28697]              : (-0.4004042867585139, 5.730138789796395)\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : tensorboardX\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n\u001b[36m─────────────────────────────────────────────── \u001b[0m\u001b[1;36mNew Comet feature!\u001b[0m\u001b[36m ───────────────────────────────────────────────\u001b[0m\nLog your models to better track, deploy, share, and reproduce your work using: 'comet_ml.integration.pytorch.log_model'.\nLearn more at: https://comet.com/docs/v2/pytorch_log_model\n\nHide this message by setting environment variable \"COMET_DISABLE_ANNOUNCEMENT=1\" \n\u001b[36m──────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/katyanaveka/actor-critic-gae/4dd1d8e77bc34a7486eb3c299665635c\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Helper function to display logged assets in the Comet UI\nimport comet_ml\ndef display(tab=None):\n    experiment = comet_ml.get_global_experiment()\n    experiment.display(tab=tab)\n    \nwrite = SummaryWriter(comet_config={\"disabled\": False})","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:29.079251Z","iopub.execute_input":"2023-04-16T15:29:29.080032Z","iopub.status.idle":"2023-04-16T15:29:29.092753Z","shell.execute_reply.started":"2023-04-16T15:29:29.079987Z","shell.execute_reply":"2023-04-16T15:29:29.091480Z"},"trusted":true},"execution_count":460,"outputs":[]},{"cell_type":"markdown","source":"In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n\nFirstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames, stack them together, prepares for PyTorch and normalizes to [0, 1]) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\nFile `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function.","metadata":{}},{"cell_type":"code","source":"import multiprocessing\n\nmultiprocessing.cpu_count()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:29.094923Z","iopub.execute_input":"2023-04-16T15:29:29.098906Z","iopub.status.idle":"2023-04-16T15:29:29.109945Z","shell.execute_reply.started":"2023-04-16T15:29:29.098864Z","shell.execute_reply":"2023-04-16T15:29:29.108615Z"},"trusted":true},"execution_count":461,"outputs":[{"execution_count":461,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom atari_wrappers import nature_dqn_env\n\nnenvs = 8    # change this if you have more than 8 CPU ;)\n\nenv = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs)\n\nn_actions = env.action_space.spaces[0].n\nobs = env.reset()\nassert obs.shape == (nenvs, 4, 84, 84)\nassert obs.dtype == np.float32","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:29.118005Z","iopub.execute_input":"2023-04-16T15:29:29.118696Z","iopub.status.idle":"2023-04-16T15:29:32.713770Z","shell.execute_reply.started":"2023-04-16T15:29:29.118656Z","shell.execute_reply":"2023-04-16T15:29:32.711821Z"},"trusted":true},"execution_count":462,"outputs":[{"name":"stderr","text":"Process Process-70:\nProcess Process-66:\nProcess Process-65:\nProcess Process-69:\nProcess Process-72:\nProcess Process-71:\nProcess Process-67:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\nTraceback (most recent call last):\nProcess Process-68:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/input/utils/env_batch.py\", line 127, in worker\n    cmd, action = worker_connection.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/kaggle/input/utils/env_batch.py\", line 129, in worker\n    ob, rew, done, info = env.step(action)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/kaggle/input/utils/env_batch.py\", line 127, in worker\n    cmd, action = worker_connection.recv()\n  File \"/kaggle/input/utils/env_batch.py\", line 129, in worker\n    ob, rew, done, info = env.step(action)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/kaggle/input/utils/env_batch.py\", line 127, in worker\n    cmd, action = worker_connection.recv()\n  File \"/kaggle/input/utils/env_batch.py\", line 127, in worker\n    cmd, action = worker_connection.recv()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/kaggle/input/utils/env_batch.py\", line 127, in worker\n    cmd, action = worker_connection.recv()\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/kaggle/input/utils/env_batch.py\", line 131, in worker\n    ob = env.reset()\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 319, in reset\n    observation = self.env.reset(**kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 174, in reset\n    obs = self.env.reset()\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 194, in step\n    obs, rew, done, info = self.env.step(action)\nKeyboardInterrupt\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 194, in step\n    obs, rew, done, info = self.env.step(action)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 319, in reset\n    observation = self.env.reset(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 324, in step\n    return self.observation(observation), reward, done, info\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/site-packages/gym/core.py\", line 323, in step\n    observation, reward, done, info = self.env.step(action)\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 144, in observation\n    obs = np.maximum(observation, self.last_obs)\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 201, in reset\n    return self.env.reset(**kwargs)\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 25, in step\n    def step(self, action):\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 149, in reset\n    self.last_obs = self.env.reset()\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 37, in reset\n    obs = self.env.reset(**kwargs)\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 92, in reset\n    obs = self.env.reset()\n  File \"/kaggle/input/utils/atari_wrappers.py\", line 67, in reset\n    self.env.reset(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/wrappers/time_limit.py\", line 27, in reset\n    return self.env.reset(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/gym/envs/atari/environment.py\", line 259, in reset\n    self.ale.reset_game()\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, we will need to implement a model that predicts logits of policy distribution and critic value. Use shared backbone. You may use same architecture as in DQN task with one modification: instead of having a single output layer, it must have two output layers taking as input the output of the last hidden layer (one for actor, one for critic). \n\nStill it may be very helpful to make more changes:\n* use orthogonal initialization with gain $\\sqrt{2}$ and initialize biases with zeros;\n* use more filters (e.g. 32-64-64 instead of 16-32-64);\n* use two-layer heads for actor and critic or add a linear layer into backbone;\n\n**Danger:** do not divide on 255, input is already normalized to [0, 1] in our wrappers!","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ACnet(nn.Module):\n    '''\n    input:\n        states - tensor, (batch_size x channels x width x height)\n    output:\n        logits - tensor, logits of action probabilities for your actor policy, (batch_size x num_actions)\n        V - tensor, critic estimation, (batch_size)\n    '''\n    def __init__(self, state_shape, n_actions, device='cpu'):\n        super().__init__()\n        self.state_shape = state_shape\n        self.n_actions = n_actions\n        self.device = device\n        self.conv = nn.Sequential(\n              nn.Conv2d(in_channels=state_shape[1], out_channels=32, kernel_size=3, stride=2),\n              nn.ReLU(),\n              nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n              nn.ReLU(),\n              nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2),\n              nn.ReLU(),\n              nn.Flatten()\n          )\n        # actor\n        self.classif_v = nn.Sequential(\n              nn.Linear(in_features = 5184, out_features = 256),\n              nn.ReLU(),\n              nn.Linear(in_features = 256, out_features = n_actions),\n              nn.Softmax(dim=1)\n          ) \n        # critic\n        self.classif_Ev = nn.Sequential(\n              nn.Linear(in_features = 5184, out_features = 256),\n              nn.ReLU(),\n              nn.Linear(in_features = 256, out_features = 1)\n          )\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=1.0)\n            torch.nn.init.orthogonal_(module.weight.data, gain = 2 ** 0.5)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n        \n    def forward(self, states):\n        pre_v = self.conv(states)\n        logits = self.classif_v(pre_v)\n        values = self.classif_Ev(pre_v)\n        return logits, values","metadata":{"execution":{"iopub.status.busy":"2023-04-25T12:46:00.617376Z","iopub.execute_input":"2023-04-25T12:46:00.617935Z","iopub.status.idle":"2023-04-25T12:46:04.190884Z","shell.execute_reply.started":"2023-04-25T12:46:00.617899Z","shell.execute_reply":"2023-04-25T12:46:04.189648Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a **dictionary** of all the arrays that are needed to interact with an environment and train the model.\n\n**Important**: \"actions\" will be sent to environment, they must be numpy array or list, not PyTorch tensor.\n\nNote: you can add more keys, e.g. it can be convenient to compute entropy right here.","metadata":{}},{"cell_type":"code","source":"from torch.distributions import Categorical\n\nclass Policy:\n    def __init__(self, model):\n        self.model = model\n\n    def act(self, inputs):\n        '''\n        input:\n            inputs - numpy array, (batch_size x channels x width x height)\n        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n            'actions' - selected actions, numpy, (batch_size)\n            'logits' - actions logits, tensor, (batch_size x num_actions)\n            'log_probs' - log probs of selected actions, tensor, (batch_size)\n            'values' - critic estimations, tensor, (batch_size)\n        '''\n        model = self.model\n        logits, values = model(torch.tensor(inputs).to(model.device))\n        cat_distr = Categorical(logits)\n        actions = cat_distr.sample()\n        log_probs = cat_distr.log_prob(actions)\n        entropy = cat_distr.entropy() # tensor, (batch_size)\n        \n        return {\n            \"actions\": actions.detach().cpu().numpy(),\n            \"logits\": logits,\n            \"log_probs\": log_probs.squeeze(0),\n            \"values\": values.squeeze(1),\n            \"entropy\": entropy\n        }","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:32.765991Z","iopub.execute_input":"2023-04-16T15:29:32.768773Z","iopub.status.idle":"2023-04-16T15:29:32.781532Z","shell.execute_reply.started":"2023-04-16T15:29:32.768735Z","shell.execute_reply":"2023-04-16T15:29:32.780331Z"},"trusted":true},"execution_count":465,"outputs":[]},{"cell_type":"markdown","source":"Next we will pass the environment and policy to a runner that collects rollouts from the environment. \nThe class is already implemented for you.","metadata":{}},{"cell_type":"code","source":"from runners import EnvRunner","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:32.787151Z","iopub.execute_input":"2023-04-16T15:29:32.790107Z","iopub.status.idle":"2023-04-16T15:29:32.797158Z","shell.execute_reply.started":"2023-04-16T15:29:32.790068Z","shell.execute_reply":"2023-04-16T15:29:32.795907Z"},"trusted":true},"execution_count":466,"outputs":[]},{"cell_type":"markdown","source":"This runner interacts with the environment for a given number of steps and returns a dictionary containing\nkeys \n\n* 'observations' \n* 'rewards' \n* 'dones'\n* 'actions'\n* all other keys that you defined in `Policy`\n\nunder each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory, or rollout length. Let's have a look at how it works.","metadata":{}},{"cell_type":"code","source":"model = ACnet(obs.shape, n_actions)\npolicy = Policy(model)\nrunner = EnvRunner(env, policy, nsteps=5)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:32.802582Z","iopub.execute_input":"2023-04-16T15:29:32.805278Z","iopub.status.idle":"2023-04-16T15:29:33.071783Z","shell.execute_reply.started":"2023-04-16T15:29:32.805240Z","shell.execute_reply":"2023-04-16T15:29:33.070467Z"},"trusted":true},"execution_count":467,"outputs":[]},{"cell_type":"code","source":"# generates new rollout\ntrajectory = runner.get_next()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:33.080068Z","iopub.execute_input":"2023-04-16T15:29:33.082767Z","iopub.status.idle":"2023-04-16T15:29:33.247168Z","shell.execute_reply.started":"2023-04-16T15:29:33.082724Z","shell.execute_reply":"2023-04-16T15:29:33.245984Z"},"trusted":true},"execution_count":468,"outputs":[]},{"cell_type":"code","source":"# what is inside\nprint(trajectory.keys())","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:33.273980Z","iopub.execute_input":"2023-04-16T15:29:33.276445Z","iopub.status.idle":"2023-04-16T15:29:33.294631Z","shell.execute_reply.started":"2023-04-16T15:29:33.276406Z","shell.execute_reply":"2023-04-16T15:29:33.293593Z"},"trusted":true},"execution_count":470,"outputs":[{"name":"stdout","text":"dict_keys(['actions', 'logits', 'log_probs', 'values', 'entropy', 'observations', 'rewards', 'dones'])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sanity checks\nassert 'logits' in trajectory, \"Not found: policy didn't provide logits\"\nassert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\nassert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\nassert trajectory['logits'][0].shape == (nenvs, n_actions), \"logits wrong shape\"\nassert trajectory['log_probs'][0].shape == (nenvs,), \"log_probs wrong shape\"\nassert trajectory['values'][0].shape == (nenvs,), \"values wrong shape\"\n\nfor key in trajectory.keys():\n    assert len(trajectory[key]) == 5, \\\n    f\"something went wrong: 5 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\"","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:33.297819Z","iopub.execute_input":"2023-04-16T15:29:33.298545Z","iopub.status.idle":"2023-04-16T15:29:33.311611Z","shell.execute_reply.started":"2023-04-16T15:29:33.298496Z","shell.execute_reply":"2023-04-16T15:29:33.310461Z"},"trusted":true},"execution_count":471,"outputs":[]},{"cell_type":"markdown","source":"Now let's work with this trajectory a bit. To train the critic you will need to compute the value targets. It will also be used as an estimation of $Q$ for actor training.\n\nYou should use all available rewards for value targets, so the formula for the value targets is simple:\n\n$$\n\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n$$\n\nwhere $s_{t + T}$ is the latest observation of the environment.\n\nAny callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \nThus, we can implement and use `ComputeValueTargets` callable. \n\n**Do not forget** to use `trajectory['dones']` flags to check if you need to add the value targets at the next step when \ncomputing value targets for the current step.\n\n**Bonus (+0.5 pts):** implement [Generalized Advantage Estimation (GAE)](https://arxiv.org/pdf/1506.02438.pdf) instead; use $\\lambda \\approx 0.95$ or even closer to 1 in experiment. ","metadata":{}},{"cell_type":"code","source":"class ComputeValueTargets:\n    def __init__(self, policy, gamma=0.99, GAE=False):\n        self.policy = policy\n        self.gamma = gamma\n        self.GAE = GAE\n\n    def __call__(self, trajectory, latest_observation):\n        '''\n        This method should modify trajectory inplace by adding \n        an item with key 'value_targets' to it\n        \n        input:\n            trajectory - dict from runner\n            latest_observation - last state, numpy, (num_envs x channels x width x height)\n        '''\n        device = self.policy.model.device\n        gamma = self.gamma\n        next_tr = self.policy.act(latest_observation)\n            \n        is_done = torch.tensor(trajectory['dones']).to(device)\n        rewards = torch.tensor(trajectory['rewards']).to(device)\n        T = len(rewards)\n        cumulative_rewards = [0 for i in range(T)]\n        if not self.GAE:\n            G_curr = next_tr['values'].to(device)\n            for i in range(T-1, -1,  -1):\n                G_curr = rewards[i] + gamma * G_curr * (~ is_done[i])\n                cumulative_rewards[i] = G_curr\n            \n        else:\n            factor = gamma * 0.95 # * lambda\n            G_curr = 0#rewards - gamma * next_tr['values'].cpu() + trajectory['values'][0] # \\psi_1\n            v = trajectory['values'].copy()\n            v.append(next_tr['values'])\n            for i in range(T-1, -1,  -1):\n                G_curr = rewards[i] + gamma * v[i+1].to(device) - v[i].to(device) + factor * G_curr * (~is_done[i])\n                cumulative_rewards[i] = G_curr\n                \n        trajectory['value_targets'] = cumulative_rewards","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:33.315823Z","iopub.execute_input":"2023-04-16T15:29:33.316689Z","iopub.status.idle":"2023-04-16T15:29:33.337370Z","shell.execute_reply.started":"2023-04-16T15:29:33.316652Z","shell.execute_reply":"2023-04-16T15:29:33.336046Z"},"trusted":true},"execution_count":472,"outputs":[]},{"cell_type":"markdown","source":"\n\nAfter computing value targets we will transform lists of interactions into tensors\nwith the first dimension `batch_size` which is equal to `T * nenvs`.\n\nYou need to make sure that after this transformation `\"log_probs\"`, `\"value_targets\"`, `\"values\"` are 1-dimensional PyTorch tensors.","metadata":{}},{"cell_type":"code","source":"class MergeTimeBatch:\n    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n    def __call__(self, trajectory, latest_observation):\n        # Modify trajectory inplace.\n        T = len(trajectory['rewards'])\n        nenvs = trajectory['rewards'][0].shape[0]\n        for key, item in trajectory.items():\n            if key in {\"log_probs\", \"values\", \"value_targets\", \"entropy\"}:\n                trajectory[key] = torch.stack(trajectory[key]).view(T * nenvs)\n            if key == 'fff':\n                trajectory[key] = torch.tensor(trajectory[key]).view(T * nenvs)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:31.015947Z","iopub.execute_input":"2023-04-16T15:30:31.016706Z","iopub.status.idle":"2023-04-16T15:30:31.024547Z","shell.execute_reply.started":"2023-04-16T15:30:31.016658Z","shell.execute_reply":"2023-04-16T15:30:31.023287Z"},"trusted":true},"execution_count":482,"outputs":[]},{"cell_type":"markdown","source":"Let's do more sanity checks!","metadata":{}},{"cell_type":"code","source":"runner = EnvRunner(env, policy, nsteps=5, transforms=[ComputeValueTargets(policy, GAE=True),\n                                                      MergeTimeBatch()])\n\ntrajectory = runner.get_next()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:32.111292Z","iopub.execute_input":"2023-04-16T15:30:32.111682Z","iopub.status.idle":"2023-04-16T15:30:32.198382Z","shell.execute_reply.started":"2023-04-16T15:30:32.111646Z","shell.execute_reply":"2023-04-16T15:30:32.197317Z"},"trusted":true},"execution_count":483,"outputs":[]},{"cell_type":"code","source":"# More sanity checks\nassert 'value_targets' in trajectory, \"Value targets not found\"\nassert trajectory['log_probs'].shape == (5 * nenvs,)\nassert trajectory['value_targets'].shape == (5 * nenvs,)\nassert trajectory['values'].shape == (5 * nenvs,)\n\nassert trajectory['log_probs'].requires_grad, \"Gradients are not available for actor head!\"\nassert trajectory['values'].requires_grad, \"Gradients are not available for critic head!\"","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:41.326677Z","iopub.execute_input":"2023-04-16T15:30:41.327057Z","iopub.status.idle":"2023-04-16T15:30:41.333431Z","shell.execute_reply.started":"2023-04-16T15:30:41.327022Z","shell.execute_reply":"2023-04-16T15:30:41.332109Z"},"trusted":true},"execution_count":486,"outputs":[]},{"cell_type":"markdown","source":"Now is the time to implement the advantage actor critic algorithm itself. You can look into [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and lectures ([part 1](https://www.youtube.com/watch?v=Ds1trXd6pos&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=5), [part 2](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=6)) by Sergey Levine.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom torch.nn.utils import clip_grad_norm_\n\nclass A2C:\n    def __init__(self, policy, optimizer, device = 'cpu', value_loss_coef=0.25, entropy_coef=0.01, max_grad_norm=0.5):\n        self.policy = policy\n        self.optimizer = optimizer\n        self.device = device\n        self.value_loss_coef = value_loss_coef\n        self.entropy_coef = entropy_coef\n        self.max_grad_norm = max_grad_norm\n        self.writer = runner.write #SummaryWriter(comet_config={\"disabled\": False})\n    \n    def loss(self, trajectory, write):\n        # compute all losses\n        # do not forget to use weights for critic loss and entropy loss\n        # print(trajectory['value_targets'].device, trajectory['values'].device)\n        if not GAE:\n            adv = trajectory['value_targets'].to(self.device) - trajectory['values']\n        else:\n            adv = trajectory['value_targets'].to(self.device)\n        critic_loss = torch.mean(adv ** 2)\n        \n        policy_loss = torch.mean(trajectory['log_probs'] * adv)\n        \n        entropy_loss = - torch.mean(trajectory['entropy'])\n        \n        \n        # log all losses\n        self.writer('losses', {\n            'policy loss': policy_loss.detach(),\n            'critic loss': critic_loss.detach(),\n            'entropy loss': entropy_loss.detach()\n        })\n        \n        # additional logs\n        self.writer('critic/advantage', adv.detach())\n        self.writer('critic/values', {\n            'value predictions': torch.mean(trajectory['values']),\n            'value targets':     torch.mean(trajectory['value_targets']),\n        })\n        \n        # return scalar loss\n        return - policy_loss + self.value_loss_coef * critic_loss + self.entropy_coef * entropy_loss\n\n    def train(self, runner):\n        # collect trajectory using runner\n        trajectory = runner.get_next()\n        # compute loss and perform one step of gradient optimization\n        loss = self.loss(trajectory, self.writer)\n        # do not forget to clip gradients\n        \n        loss.backward()\n        grad_norm = nn.utils.clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # use runner.write to log scalar to tensorboard\n        self.writer('gradient norm', grad_norm)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:41.759174Z","iopub.execute_input":"2023-04-16T15:30:41.760854Z","iopub.status.idle":"2023-04-16T15:30:41.773639Z","shell.execute_reply.started":"2023-04-16T15:30:41.760805Z","shell.execute_reply":"2023-04-16T15:30:41.772533Z"},"trusted":true},"execution_count":487,"outputs":[]},{"cell_type":"markdown","source":"Now you can train your model. For optimization we suggest you use RMSProp with learning rate 7e-4 (you can also linearly decay it to 0), smoothing constant (alpha in PyTorch) equal to 0.99 and epsilon equal to 1e-5.\n\nWe recommend to train for at least 10 million environment steps across all batched environments (takes ~3 hours on a single GTX1080 with 8 CPU). It should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last episodes in each environment in the batch) of about 600. **Your goal is to reach 500**.\n\nNotes:\n* if your reward is stuck at ~200 for more than 2M steps then probably there is a bug\n* if your gradient norm is >10 something probably went wrong\n* make sure your `entropy loss` is negative, your `critic loss` is positive\n* make sure you didn't forget `.detach` in losses where it's needed\n* `actor loss` should oscillate around zero or near it; do not expect loss to decrease in RL ;)\n* you can experiment with `nsteps` (\"rollout length\"); standard rollout length is 5 or 10. Note that this parameter influences how many algorithm iterations is required to train on 10M steps (or 40M frames --- we used frameskip in preprocessing).","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nGAE = True\n\nmodel = ACnet(obs.shape, n_actions, device)\npolicy = Policy(model)\nrunner = EnvRunner(env, policy, nsteps=10, transforms=[ComputeValueTargets(policy, GAE=GAE),\n                                                      MergeTimeBatch()])\n\noptimizer = torch.optim.RMSprop(model.parameters(), lr=7e-4, eps=1e-5)\n\na2c = A2C(policy, optimizer, device)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:42.925860Z","iopub.execute_input":"2023-04-16T15:30:42.926233Z","iopub.status.idle":"2023-04-16T15:30:43.043421Z","shell.execute_reply.started":"2023-04-16T15:30:42.926197Z","shell.execute_reply":"2023-04-16T15:30:43.042378Z"},"trusted":true},"execution_count":488,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:43.148583Z","iopub.execute_input":"2023-04-16T15:30:43.149422Z","iopub.status.idle":"2023-04-16T15:30:43.158537Z","shell.execute_reply.started":"2023-04-16T15:30:43.149374Z","shell.execute_reply":"2023-04-16T15:30:43.157328Z"},"trusted":true},"execution_count":489,"outputs":[]},{"cell_type":"code","source":"from tqdm import trange\ntotal_steps = 10 * 10**6\n\n# runner.reset()\nfor i in trange(total_steps):\n    a2c.train(runner) # KA73UNUPM3eR56uID83Ii8HH4","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:30:44.025439Z","iopub.execute_input":"2023-04-16T15:30:44.026135Z","iopub.status.idle":"2023-04-16T21:44:04.096406Z","shell.execute_reply.started":"2023-04-16T15:30:44.026095Z","shell.execute_reply":"2023-04-16T21:44:04.074618Z"},"trusted":true},"execution_count":490,"outputs":[{"name":"stderr","text":"  0%|          | 0/10000000 [00:00<?, ?it/s]\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/katyanaveka/actor-critic-gae/4dd1d8e77bc34a7486eb3c299665635c\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/katyanaveka/general/679a648b4c3641f09c4016c04bd1dbf0\n\n  1%|▏         | 126071/10000000 [6:13:19<487:19:00,  5.63it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/333749828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# runner.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0ma2c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# KA73UNUPM3eR56uID83Ii8HH4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/552268468.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, runner)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# collect trajectory using runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# compute loss and perform one step of gradient optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/utils/runners.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/utils/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_ended_episodes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/utils/env_batch.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/utils/env_batch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;34m\"\"\"Receive a (picklable) object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_check_closed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"handle is closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"Experiment.end(experiment)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T15:29:11.863077Z","iopub.execute_input":"2023-04-16T15:29:11.863447Z","iopub.status.idle":"2023-04-16T15:29:12.356483Z","shell.execute_reply.started":"2023-04-16T15:29:11.863414Z","shell.execute_reply":"2023-04-16T15:29:12.355437Z"},"trusted":true},"execution_count":458,"outputs":[]},{"cell_type":"code","source":"# save your model just in case \ntorch.save(model.state_dict(), \"A2C_GAE_rmsprop\")  ","metadata":{"execution":{"iopub.status.busy":"2023-04-16T21:44:19.782195Z","iopub.execute_input":"2023-04-16T21:44:19.783114Z","iopub.status.idle":"2023-04-16T21:44:19.833872Z","shell.execute_reply.started":"2023-04-16T21:44:19.783071Z","shell.execute_reply":"2023-04-16T21:44:19.832844Z"},"trusted":true},"execution_count":491,"outputs":[]},{"cell_type":"code","source":"env.close()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T21:44:23.035658Z","iopub.execute_input":"2023-04-16T21:44:23.036387Z","iopub.status.idle":"2023-04-16T21:44:23.060888Z","shell.execute_reply.started":"2023-04-16T21:44:23.036347Z","shell.execute_reply":"2023-04-16T21:44:23.059589Z"},"trusted":true},"execution_count":492,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3751641700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/utils/env_batch.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_connections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"close\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"],"ename":"BrokenPipeError","evalue":"[Errno 32] Broken pipe","output_type":"error"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, \n                     clip_reward=False, summaries=False, episodic_life=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T21:44:24.594045Z","iopub.execute_input":"2023-04-16T21:44:24.594386Z","iopub.status.idle":"2023-04-16T21:44:24.901232Z","shell.execute_reply.started":"2023-04-16T21:44:24.594355Z","shell.execute_reply":"2023-04-16T21:44:24.900010Z"},"trusted":true},"execution_count":493,"outputs":[]},{"cell_type":"code","source":"def evaluate(env, policy, n_games=1, t_max=10000):\n    '''\n    Plays n_games and returns rewards\n    '''\n    rewards = []\n    \n    for _ in range(n_games):\n        s = env.reset()\n        \n        R = 0\n        for _ in range(t_max):\n            action = policy.act(np.array([s]))[\"actions\"][0]\n            \n            s, r, done, _ = env.step(action)\n            \n            R += r\n            if done:\n                break\n\n        rewards.append(R)\n    return np.array(rewards)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T21:44:25.611646Z","iopub.execute_input":"2023-04-16T21:44:25.612007Z","iopub.status.idle":"2023-04-16T21:44:25.619518Z","shell.execute_reply.started":"2023-04-16T21:44:25.611972Z","shell.execute_reply":"2023-04-16T21:44:25.618447Z"},"trusted":true},"execution_count":494,"outputs":[]},{"cell_type":"code","source":"# evaluation will take some time!\nsessions = evaluate(env, policy, n_games=30)\nscore = sessions.mean()\nprint(f\"Your score: {score}\")\n\nassert score >= 500, \"Needs more training?\"\nprint(\"Well done!\")","metadata":{"execution":{"iopub.status.busy":"2023-04-16T21:44:26.388804Z","iopub.execute_input":"2023-04-16T21:44:26.389110Z","iopub.status.idle":"2023-04-16T21:45:28.419074Z","shell.execute_reply.started":"2023-04-16T21:44:26.389081Z","shell.execute_reply":"2023-04-16T21:45:28.417718Z"},"trusted":true},"execution_count":495,"outputs":[{"name":"stdout","text":"Your score: 245.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1519910363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Your score: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Needs more training?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Well done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Needs more training?"],"ename":"AssertionError","evalue":"Needs more training?","output_type":"error"}]},{"cell_type":"code","source":"env.close()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T08:59:45.455920Z","iopub.status.idle":"2023-04-16T08:59:45.457072Z","shell.execute_reply.started":"2023-04-16T08:59:45.456807Z","shell.execute_reply":"2023-04-16T08:59:45.456835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Record","metadata":{}},{"cell_type":"code","source":"env_monitor = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, monitor=True,\n                             clip_reward=False, summaries=False, episodic_life=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# record sessions\nsessions = evaluate(env_monitor, policy, n_games=3)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rewards for recorded games\nsessions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env_monitor.close()","metadata":{},"execution_count":null,"outputs":[]}]}